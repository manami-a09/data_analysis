# ============================================
# Step 0ï¼šæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆColabæ–‡å­—åŒ–ã‘é˜²æ­¢ï¼‰
# ============================================
!apt-get -y install fonts-ipafont-gothic > /dev/null

import subprocess
subprocess.run(["fc-cache", "-fv"], check=True)

import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.font_manager as fm
import os

# --- ãƒ•ã‚©ãƒ³ãƒˆæ¤œå‡ºï¼†ç™»éŒ² ---
font_candidates = [f for f in fm.findSystemFonts() if "ipag" in f.lower()]
if not font_candidates:
    raise FileNotFoundError("IPAãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
font_path = font_candidates[0]
fm.fontManager.addfont(font_path)
mpl.rc("font", family="IPAGothic")
mpl.rcParams["axes.unicode_minus"] = False
print(f"âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šå®Œäº†ï¼š{os.path.basename(font_path)}")

# ============================================
# Step 1ã€œ3ï¼šå‰å‡¦ç†ã¨ãƒ¢ãƒ‡ãƒ«æº–å‚™
# ============================================
!pip install -q sentence-transformers scikit-learn pandas numpy matplotlib

import pandas as pd, numpy as np, re, random
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize

# ==== ã‚µãƒ³ãƒ—ãƒ«å–¶æ¥­ãƒ¡ãƒ¢ ====
salespersons = ["Aã•ã‚“","Bã•ã‚“","Cã•ã‚“","Dã•ã‚“","Eã•ã‚“"]
notes = [
    "æ–™é‡‘ãƒ—ãƒ©ãƒ³ã®é•ã„ã‚’èª¬æ˜ã—ã€çœã‚¨ãƒæ–½ç­–ã‚’ææ¡ˆã€‚é›»åŠ›é‡ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã«é–¢å¿ƒã‚ã‚Šã€‚",
    "ä»–ç¤¾æ¯”è¼ƒã‚’æ±‚ã‚ã‚‰ã‚ŒãŸã€‚ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœã®è©¦ç®—ã‚’æç¤ºã—ã€æ¥æœˆè¦‹ç©æå‡ºäºˆå®šã€‚",
    "ãƒˆãƒ©ãƒ–ãƒ«å¯¾å¿œã®ä¸æº€ãŒã‚ã£ãŸãŸã‚ã€ä¿å®ˆã‚µãƒãƒ¼ãƒˆã®å¼·åŒ–ç­–ã‚’èª¬æ˜ã€‚è¬ç½ªã¨æ¬¡å›ãƒ•ã‚©ãƒ­ãƒ¼ã‚’ç´„æŸã€‚",
    "å†ã‚¨ãƒãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç›¸è«‡ã€‚ç’°å¢ƒä¾¡å€¤ã¨é•·æœŸçš„ãªå˜ä¾¡ãƒªã‚¹ã‚¯ã‚’èª¬æ˜ã€‚æ„æ€æ±ºå®šè€…åŒå¸­ã‚’ä¾é ¼ã€‚",
    "åŸºæœ¬æ–™é‡‘ã®è¦‹ç›´ã—è¦æœ›ã€‚éœ€è¦å®¶å´ã®ãƒ”ãƒ¼ã‚¯ã‚«ãƒƒãƒˆææ¡ˆã€BEMSé€£æºã®äº‹ä¾‹ã‚’ç´¹ä»‹ã€‚",
    "çœã‚¨ãƒè£œåŠ©é‡‘ã®æƒ…å ±æä¾›ã€‚å°å…¥ã‚¹ãƒ†ãƒƒãƒ—ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å…±æœ‰ã—ã¦åˆæ„å½¢æˆã‚’å›³ã£ãŸã€‚",
    "å¥‘ç´„æ›´æ–°ã«å‘ã‘ãŸã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°ã€‚å¹´é–“å‰Šæ¸›è¦‹è¾¼ã¿ã‚’å†æç¤ºã—ã€ç¤¾å†…ç¨Ÿè­°ã®é€²ã‚æ–¹ã‚’åŠ©è¨€ã€‚",
    "è‹¦æƒ…å¯¾å¿œï¼šè«‹æ±‚é‡‘é¡ã®èª¤èªãŒç™ºè¦šã€‚æ˜ç´°ã®å†…è¨³ã‚’èª¬æ˜ã—ã€æ¬¡å›ã¾ã§ã«å†ç™ºé˜²æ­¢ç­–ã‚’æå‡ºã€‚",
    "éœ€è¦äºˆæ¸¬ã®è©±é¡Œã‹ã‚‰ã€ãƒ‡ãƒãƒ³ãƒ‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å¯èƒ½æ€§ã‚’æç¤ºã€‚å°è¦æ¨¡ã‹ã‚‰ã®PoCææ¡ˆã€‚",
    "æ„æ€æ±ºå®šè€…ãŒä¸åœ¨ã€‚å‚¾è´ã«å¾¹ã—ã€ç¾çŠ¶èª²é¡Œã‚’æ•´ç†ã€‚æ¬¡å›ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ã‚’åˆæ„ã€‚"
]
data = [{"salesperson": random.choice(salespersons), "note": random.choice(notes)} for _ in range(120)]
df = pd.DataFrame(data)
df["text"] = df["note"].apply(lambda s: re.sub(r"\s+", " ", s).strip())

# ==== SentenceTransformer ====
model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
embeddings = model.encode(df["text"].tolist(), show_progress_bar=True)
embeddings = normalize(embeddings)

# ============================================
# Step 4ï¼šã‚¯ãƒ©ã‚¹ã‚¿æ•°è‡ªå‹•æœ€é©åŒ–ï¼‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
# ============================================
scores = []
K_range = range(4, 9)
for k in K_range:
    km = KMeans(n_clusters=k, random_state=42, n_init="auto")
    labels = km.fit_predict(embeddings)
    score = silhouette_score(embeddings, labels)
    scores.append(score)
best_k = K_range[np.argmax(scores)]
print(f"æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {best_k}")

kmeans = KMeans(n_clusters=best_k, random_state=42, n_init="auto")
df["cluster"] = kmeans.fit_predict(embeddings)

# ============================================
# Step 5ï¼šå„ã‚¯ãƒ©ã‚¹ã‚¿ä»£è¡¨æ–‡
# ============================================
rep_texts = []
for c in range(best_k):
    idx = np.where(df["cluster"] == c)[0]
    cluster_vecs = embeddings[idx]
    centroid = cluster_vecs.mean(axis=0)
    sims = cosine_similarity(cluster_vecs, [centroid])
    rep_text = df.iloc[idx[np.argmax(sims)]]["text"]
    rep_texts.append(rep_text)
    print(f"=== Cluster {c} ã®ä»£è¡¨æ–‡ ===")
    print(rep_text)

# ============================================
# Step 6ï¼šAIã‚¹ã‚­ãƒ«å‘½åï¼ˆSentenceé¡ä¼¼åº¦ï¼‰
# ============================================
skill_def = {
    "ææ¡ˆåŠ›": "é¡§å®¢ã®èª²é¡Œã‚’ç†è§£ã—æœ€é©ãªææ¡ˆã‚’è¡Œã†ã‚¹ã‚­ãƒ«ã€‚",
    "èª²é¡Œè§£æ±ºåŠ›": "èª²é¡Œã‚’ç™ºè¦‹ã—ã€è§£æ±ºç­–ã‚’å°ãå‡ºã™ã‚¹ã‚­ãƒ«ã€‚",
    "å‚¾è´åŠ›": "é¡§å®¢ã®è©±ã‚’ä¸å¯§ã«èãã€è¦æœ›ã‚’å¼•ãå‡ºã™ã‚¹ã‚­ãƒ«ã€‚",
    "èª¬æ˜åŠ›": "å°‚é–€çš„ãªå†…å®¹ã‚’ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã™ã‚‹ã‚¹ã‚­ãƒ«ã€‚",
    "ã‚¯ãƒ¬ãƒ¼ãƒ å¯¾å¿œ": "è‹¦æƒ…ã«å†·é™ã«å¯¾å¿œã—ã€ä¿¡é ¼å›å¾©ã‚’è¡Œã†ã‚¹ã‚­ãƒ«ã€‚",
    "äº¤æ¸‰åŠ›": "æ¡ä»¶äº¤æ¸‰ã‚’å††æ»‘ã«é€²ã‚ã‚‹ã‚¹ã‚­ãƒ«ã€‚",
    "ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°": "å•†è«‡ã‚’ã¾ã¨ã‚å¥‘ç´„ã«å°ãã‚¹ã‚­ãƒ«ã€‚",
    "é–¢ä¿‚æ§‹ç¯‰åŠ›": "é•·æœŸçš„ãªä¿¡é ¼é–¢ä¿‚ã‚’ç¯‰ãã‚¹ã‚­ãƒ«ã€‚",
    "ãƒãƒ¼ãƒ é€£æºåŠ›": "ç¤¾å†…å¤–ã§å”åŠ›ã—èª²é¡Œã‚’è§£æ±ºã™ã‚‹ã‚¹ã‚­ãƒ«ã€‚",
    "é¡§å®¢ç†è§£åŠ›": "é¡§å®¢ã®æ¥­ç¨®ã‚„èƒŒæ™¯ã‚’æŠŠæ¡ã—ã€æœ€é©ãªå¯¾å¿œã‚’è¡Œã†ã‚¹ã‚­ãƒ«ã€‚",
}
skill_keys = list(skill_def.keys())
skill_texts = list(skill_def.values())

cluster_emb = model.encode(rep_texts)
skill_emb = model.encode(skill_texts)
sim = cosine_similarity(cluster_emb, skill_emb)

auto_skill_names, auto_scores = [], []
for i in range(best_k):
    idx = np.argmax(sim[i])
    score = sim[i, idx]
    name = skill_keys[idx] if score >= 0.35 else "ãã®ä»–"
    auto_skill_names.append(name)
    auto_scores.append(score)

# é‡è¤‡æ•´ç†
used = set()
for i in range(best_k):
    if auto_skill_names[i] in used and auto_skill_names[i] != "ãã®ä»–":
        auto_skill_names[i] = "ãã®ä»–"
    else:
        used.add(auto_skill_names[i])

print("\n=== ğŸ§  è‡ªå‹•ã‚¹ã‚­ãƒ«ã‚«ãƒ†ã‚´ãƒªå ===")
for i, (name, sc) in enumerate(zip(auto_skill_names, auto_scores)):
    print(f"Cluster {i} â†’ {name}ï¼ˆé¡ä¼¼åº¦: {sc:.2f}ï¼‰")

df["cluster_name"] = df["cluster"].apply(lambda x: auto_skill_names[int(x)])

# ============================================
# Step 7ï¼šãã®ä»–å†åˆ†é¡ï¼‹å†ã‚°ãƒ©ãƒ•
# ============================================
other_texts = df[df["cluster_name"] == "ãã®ä»–"]["text"].tolist()
if len(other_texts) > 10:
    print("\n=== ãã®ä»–ã‚¯ãƒ©ã‚¹ã‚¿å†åˆ†æ ===")
    other_emb = model.encode(other_texts)
    sub_k = min(3, len(other_texts))
    sub_kmeans = KMeans(n_clusters=sub_k, random_state=42, n_init="auto")
    sub_labels = sub_kmeans.fit_predict(other_emb)

    refined_labels, sub_rep_texts = [], []
    for i in range(sub_k):
        idx = np.where(sub_labels == i)[0]
        centroid = other_emb[idx].mean(axis=0)
        sims = cosine_similarity(other_emb[idx], [centroid])
        rep_text = other_texts[idx[np.argmax(sims)]]
        sub_rep_texts.append(rep_text)
        print(f"\n--- ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã‚¿{i} ä»£è¡¨æ–‡ ---\n{rep_text}")

    sub_rep_emb = model.encode(sub_rep_texts)
    sim_sub = cosine_similarity(sub_rep_emb, skill_emb)

    for i, t in enumerate(sub_rep_texts):
        best_skill_idx = np.argmax(sim_sub[i])
        score = sim_sub[i, best_skill_idx]
        if score > 0.4:
            refined_labels.append(skill_keys[best_skill_idx])
        else:
            refined_labels.append(f"{t[:5]}å¯¾å¿œåŠ›")

    # å†åˆ†é¡ã‚’åæ˜ ï¼ˆdfæœ¬ä½“ã«ä¸Šæ›¸ãï¼‰
    df_other = df[df["cluster_name"] == "ãã®ä»–"].copy()
    df_other_emb = model.encode(df_other["text"].tolist())
    df_other["sub_cluster"] = sub_kmeans.fit_predict(df_other_emb)
    df_other["cluster_name"] = df_other["sub_cluster"].apply(lambda x: refined_labels[x])

    df = pd.concat([
        df[df["cluster_name"] != "ãã®ä»–"],
        df_other
    ], ignore_index=True)

    print("âœ… ãã®ä»–ã‚’å†åˆ†é¡ã—ã€dfã«çµ±åˆã—ã¾ã—ãŸã€‚")
else:
    print("\nâšª ãã®ä»–ãŒå°‘ãªã„ãŸã‚å†ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

# ============================================
# Step 8ï¼šæœ€çµ‚ã‚°ãƒ©ãƒ•å‡ºåŠ›ï¼ˆå†åˆ†é¡å¾Œï¼‰
# ============================================
pivot_final = df.groupby(["salesperson", "cluster_name"]).size().unstack(fill_value=0)
pivot_final.plot(kind="bar", figsize=(10,6))
plt.title("æ‹…å½“è€… Ã— æœ€çµ‚ã‚¹ã‚­ãƒ«ã‚«ãƒ†ã‚´ãƒªï¼ˆå†åˆ†é¡å¾Œï¼‰", fontsize=14)
plt.xlabel("æ‹…å½“è€…", fontsize=12)
plt.ylabel("ä»¶æ•°", fontsize=12)
plt.legend(title="ã‚¹ã‚­ãƒ«ã‚«ãƒ†ã‚´ãƒª", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()
