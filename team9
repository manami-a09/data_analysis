# =====================================================
# ğŸš€ Colabæœ€çµ‚å®Œæˆç‰ˆï¼šæ„å‘³ãƒ»æœŸé–“ãƒ»å‰å¾Œæ–‡ã‚’è€ƒæ…®ã—ãŸæ—¥æœ¬èªæ¤œç´¢ãƒ„ãƒ¼ãƒ«
# =====================================================
!pip install rank-bm25 fugashi ipadic scikit-learn tqdm ipywidgets pandas sentence-transformers --quiet

import re
import pandas as pd
from pathlib import Path
from collections import Counter
from typing import List, Dict, Any
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from rank_bm25 import BM25Okapi
import fugashi, ipadic
import ipywidgets as widgets
from IPython.display import display
from sentence_transformers import SentenceTransformer, util

# --- åˆæœŸåŒ– ---
tagger = fugashi.GenericTagger(ipadic.MECAB_ARGS)
embedder = SentenceTransformer("sonoisa/sentence-bert-base-ja-mean-tokens")
DATA_DIR = Path("data"); DATA_DIR.mkdir(exist_ok=True)
print("âœ… MeCabåˆæœŸåŒ–å®Œäº†ï¼ˆUTF-8è¾æ›¸ï¼‰")
print("âœ… Sentence-BERTèª­è¾¼å®Œäº†ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰")

# =====================================================
# Step 1: ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ç”Ÿæˆ
# =====================================================
def make_sample_docs():
    docs = {
        "å–¶æ¥­æ–½ç­–_çœã‚¨ãƒè£œåŠ©.txt": (
            "2025å¹´åº¦ çœã‚¨ãƒè£œåŠ©é‡‘ã®ç”³è«‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæƒ³å®šï¼‰ã€‚\n"
            "ä¸€æ¬¡å…¬å‹Ÿï¼š2æœˆä¸Šæ—¬ï½3æœˆä¸­æ—¬ã€‚\n"
            "äºŒæ¬¡å…¬å‹Ÿï¼š5æœˆï½6æœˆã€‚\n"
            "ä¸Šé™1,500ä¸‡å††ã€è£œåŠ©ç‡1/2ã€‚\n"
            "å¯¾è±¡ï¼šé«˜åŠ¹ç‡ç©ºèª¿ã€ã‚¤ãƒ³ãƒãƒ¼ã‚¿ã€LEDæ›´æ–°ã€BEMSç­‰ã€‚\n"
            "å–¶æ¥­ã¯é«˜åœ§éœ€è¦å®¶ã®å†·å‡å†·è”µãƒ»ç©ºèª¿æ›´æ–°ã®æ˜ã‚Šèµ·ã“ã—ã‚’å„ªå…ˆã€‚\n"
            "çœã‚¨ãƒè¨ºæ–­ã®ç„¡æ–™ã‚¯ãƒ¼ãƒãƒ³ã‚’é…å¸ƒã€‚\n"
        ),
        "æ–™é‡‘æ”¹å®š_2025Q1.txt": (
            "2025å¹´1æœˆå®Ÿæ–½ã®æ–™é‡‘æ”¹å®šã«é–¢ã™ã‚‹ç¤¾å†…å‘ã‘å‘¨çŸ¥ã€‚\n"
            "éœ€è¦å®¶ã¸ã®å½±éŸ¿ã¯å¹³å‡+3.2%ã€‚ä½åœ§ã¯+2.0%ã€é«˜åœ§ã¯+3.8%ã€ç‰¹åˆ¥é«˜åœ§ã¯+4.1%ã€‚\n"
            "ç‡ƒæ–™è²»èª¿æ•´å˜ä¾¡ã®è¦‹ç›´ã—ã¨å†ã‚¨ãƒè³¦èª²é‡‘ã®å¢—é¡ãŒä¸»å› ã€‚\n"
            "å–¶æ¥­éƒ¨ã¯ä¸­å°ä¼æ¥­å‘ã‘ã®èª¬æ˜è³‡æ–™ï¼ˆQ&Aï¼‰ã‚’1æœˆ10æ—¥ã¾ã§ã«æå‡ºã€‚\n"
        ),
        "QandA_æ–™é‡‘æ”¹å®š.txt": (
            "Q: æ–™é‡‘æ”¹å®šã®ç†ç”±ã¯ï¼Ÿ\n"
            "A: å›½éš›ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¾¡æ ¼ã®é«˜æ­¢ã¾ã‚Šã€ç‚ºæ›¿ã€å†ã‚¨ãƒé–¢é€£è²»ç”¨ã®å¢—åŠ ãŒè¦å› ã§ã™ã€‚\n"
            "Q: å€¤ä¸ŠãŒã‚Šå¹…ã¯ï¼Ÿ\n"
            "A: ä½åœ§+2.0%ã€é«˜åœ§+3.8%ã€ç‰¹åˆ¥é«˜åœ§+4.1%ã®è¦‹è¾¼ã¿ã§ã™ã€‚\n"
            "Q: å½±éŸ¿ã‚’æŠ‘ãˆã‚‹æ–¹æ³•ã¯ï¼Ÿ\n"
            "A: ä½¿ç”¨æ™‚é–“å¸¯ã®ã‚·ãƒ•ãƒˆã€åŸºæœ¬æ–™é‡‘ã®è¦‹ç›´ã—ã€çœã‚¨ãƒè¨­å‚™æ›´æ–°ã®æ¤œè¨ãŒæœ‰åŠ¹ã§ã™ã€‚\n"
        )
    }
    for name, text in docs.items():
        (DATA_DIR / name).write_text(text, encoding="utf-8")
    print("âœ… ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ã‚’ data/ ã«ä½œæˆã—ã¾ã—ãŸ")

# =====================================================
# Step 2: æ–‡åˆ†å‰²ï¼†å½¢æ…‹ç´ è§£æ
# =====================================================
def read_docs() -> List[Dict[str, Any]]:
    items = []
    for p in DATA_DIR.glob("*.txt"):
        raw = p.read_text(encoding="utf-8")
        raw = raw.replace("\r\n", "ã€‚").replace("\n", "ã€‚")
        sents = re.split(r"(?<=[ã€‚ï¼!ï¼?ï¼Ÿ])", raw)
        for i, s in enumerate(sents):
            s = s.strip()
            if len(s) > 1:
                items.append({"doc": p.name, "sent_id": i, "text": s})
    return items

def tokenize_ja(text):
    return [word.surface for word in tagger(text)]

# =====================================================
# Step 3: æ¤œç´¢ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆTF-IDF + BM25ï¼‰
# =====================================================
def build_models(texts: List[str]):
    tokenized_texts = [" ".join(tokenize_ja(t)) for t in texts]
    vec = TfidfVectorizer(max_features=5000)
    X = vec.fit_transform(tokenized_texts)
    bm25 = BM25Okapi([t.split() for t in tokenized_texts])
    return vec, X, bm25

# =====================================================
# Step 4: æ¤œç´¢ï¼ˆæ„å‘³ï¼‹æœŸé–“ï¼‹å‰å¾Œæ–‡è£œå¼·ï¼‰
# =====================================================
def search_with_context(query: str, rows: List[Dict[str, Any]], vec, X, bm25, topk=5, min_match=2):
    query_tok = tokenize_ja(query)
    qv = vec.transform([" ".join(query_tok)])
    sim = cosine_similarity(qv, X).ravel()
    bm25_scores = bm25.get_scores(query_tok)

    fused = Counter()
    for i in range(len(rows)):
        fused[i] = sim[i] + bm25_scores[i]

    prelim = [i for i, _ in fused.most_common(topk*3)]
    results = []

    for i in prelim:
        text = rows[i]["text"]
        overlap = len(set(query_tok) & set(tokenize_ja(text)))
        if overlap < min_match:
            continue

        doc_name = rows[i]["doc"]
        doc_rows = [r["text"] for r in rows if r["doc"] == doc_name]

        # âœ… æ”¹è‰¯ç‰ˆï¼šé–¢é€£æ–‡ï¼‹å‰å¾Œ2æ–‡ã‚’çµåˆ
        related = []
        for j, t in enumerate(doc_rows):
            if any(w in t for w in query_tok):
                start = max(0, j - 2)
                end = min(len(doc_rows), j + 3)
                related.extend(doc_rows[start:end])
        context_text = " ".join(sorted(set(related)))

        results.append({
            "doc": doc_name,
            "text": context_text,
            "score": float(fused[i])
        })

    if not results:
        return []

    # --- æ„å‘³é¡ä¼¼åº¦ã‚’è¨ˆç®— ---
    texts = [r["text"] for r in results]
    q_emb = embedder.encode([query], convert_to_tensor=True)
    t_embs = embedder.encode(texts, convert_to_tensor=True)
    cos_scores = util.cos_sim(q_emb, t_embs)[0]
    for i, r in enumerate(results):
        r["semantic_score"] = float(cos_scores[i])
        r["final_score"] = 0.6 * r["score"] + 0.4 * r["semantic_score"]

        # âœ… æœŸé–“è¡¨ç¾ã‚’å«ã‚€æ–‡ã‚’ã‚¹ã‚³ã‚¢è£œæ­£ï¼ˆï¼‹0.5ï¼‰
        if re.search(r"\d{1,2}æœˆ|ä¸Šæ—¬|ä¸­æ—¬|ä¸‹æ—¬|å¹´åº¦|æœˆ", r["text"]):
            r["final_score"] += 0.5

    merged_results = {}
    for r in sorted(results, key=lambda x: -x["final_score"]):
        if r["doc"] not in merged_results:
            merged_results[r["doc"]] = r
        else:
            merged_results[r["doc"]]["text"] += " " + r["text"]

    return list(merged_results.values())[:topk]

# =====================================================
# Step 5: è¦ç´„ï¼ˆä¸Šä½æ–‡ã®æŠœç²‹ï¼‰
# =====================================================
def simple_summary(cands: List[Dict[str, Any]], max_sent=3):
    seen, picked = set(), []
    for c in cands:
        t = re.sub(r"\s+", "", c["text"])
        if t[:30] not in seen:
            picked.append(c["text"])
            seen.add(t[:30])
        if len(picked) >= max_sent:
            break
    return " / ".join(picked)

# =====================================================
# Step 6: UIï¼ˆå¸¸ã«å…¥åŠ›å¯èƒ½ï¼‰
# =====================================================
output_area = widgets.Output()

def run_search(query):
    with output_area:
        output_area.clear_output(wait=True)
        print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}\n")
        if not any(DATA_DIR.glob("*.txt")):
            make_sample_docs()

        rows = read_docs()
        texts = [r["text"] for r in rows]
        vec, X, bm25 = build_models(texts)
        hits = search_with_context(query, rows, vec, X, bm25, topk=5)
        if not hits:
            print("è©²å½“ã™ã‚‹æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        summary = simple_summary(hits, 3)

        df = pd.DataFrame(hits)[["doc", "text", "final_score"]]
        df["final_score"] = df["final_score"].round(3)
        display(df.style.set_properties(**{'white-space': 'pre-wrap'}))

        print("\n--- æŠ½å‡ºè¦ç´„ ---")
        print(summary)
        print("\nğŸ’¡ ã‚¯ã‚¨ãƒªã‚’å¤‰æ›´ã—ã¦å†å®Ÿè¡Œã§ãã¾ã™ã€‚")

query_box = widgets.Text(
    value='çœã‚¨ãƒ è£œåŠ©é‡‘ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«',
    placeholder='æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›',
    description='æ¤œç´¢ã‚¯ã‚¨ãƒª:',
    layout=widgets.Layout(width='70%')
)
run_button = widgets.Button(description="æ¤œç´¢å®Ÿè¡Œ", button_style='success')

def on_click(b):
    run_search(query_box.value)

run_button.on_click(on_click)
display(widgets.VBox([widgets.HBox([query_box, run_button]), output_area]))
